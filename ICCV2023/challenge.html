
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
</head>

<body>

<!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="index.html">Workshop Website</a></li>
        <li><a href="#geonet">GeoNet dataset</a></li>
        <li><a href="#details">Challenge Details</a></li>
        <li><a href="#rules">Challenge Rules</a></li>
        <li><a href="#prizes">Prizes</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>

  </div>
</div>

<div class="container">
  <div class="page-content">
      <p><br /></p>

      <div class="row">
        <div class="col-xs-12">
          <center><h1>GeoNet Challenge Details</h1></center>
          <center><h2>ICCV 2023</h2></center>
          <!-- <center>June 25, 2021</center> -->
        </div>
      </div>

<hr />

<div class="row">
    <p>
        The primary objective of the 1st GeoNet challenge is to encourage participants to put forth their ideas and algorithms aimed at addressing the issue of geographical bias by utilizing our recently introduced dataset, GeoNet. The competition consists of three different challenges, each focusing on different settings popular in modern domain adaptation. The unsupervised domain adaptation will be conducted on GeoPlaces and GeoImnet, while the universal domain adaptation will be held on GeoUniDA datasets.
    </p>
</div>

<p>
  <center>
    <b>
  <p style="color:brown;" align="center">
    Please register for the challenge at this link: <a href="https://forms.gle/zSZA1iaPD3mZxjyn7" target="_blank">https://forms.gle/zSZA1iaPD3mZxjyn7.</a> Note that registering your team is mandatory to participate in the challenge and obtain the test data. 
  </p>
</b>
</center>
</p>


<p><br /></p>
<div class="row" id="geonet">
  <div class="col-xs-12">
    <h2><u>Overview of GeoNet dataset</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
        To address the need of evaluating geographical robustness of vision models, we recently created a large-scale dataset called GeoNet which contains images from US and Asia domains covering the tasks of scene recognition (GeoPlaces) and object recognition (GeoImnet). 
        <!-- Our benchmark for place recognition, <i>GeoPlaces</i>, is sourced from the popular Places-205 dataset after segregating the images with respect to their geographies. It has more than 700k images in total across 205 categories. Similarly, the benchmark for object classification, <i>GeoImnet</i>, is built using images collected from the WebVision dataset. It consists of over 600k images across USA and Asia domains covering 700 categories.  -->
        A key facet of this dataset is the cross-geography domain shifts, along with prominent label noise in the training data. Please refer to our CVPR 2023 <a href="https://tarun005.github.io/GeoNet/">paper</a> for more details about our dataset. We envision GeoNet to provide a suitable testbed for the community to enable development of geographically robust learning algorithms. Furthermore, we also aim to spur discussion on fairness and inclusivity from the perspective of under-represented geographies. All the participants should use data only from GeoNet for the challenge.
    </p>

  </div>
</div>

<center>
  <figure>
    <img src="static/img/site/geonet_stats.png" width=300> 
    <figcaption style="font-family: Arial;"><b>Summary of GeoNet:</b> Number of images in train and test splits in each of our benchmarks.</figcaption>
  </figure>
</center>


<p><br /></p>
<div class="row" id="geonet">
  <div class="col-xs-12">
    <h2><u>Downloading the Data and Baselines</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">

    <p style="color:brown;" align="center">
        <i> You can download the dataset  <a href="https://drive.google.com/drive/folders/1x2R1AlCaww7VIrYupCxxGZdkm4nVqFLE?usp=sharing" target="_blank">here</a> </i>. The code for training baseline models on GeoNet is available at <a href="https://github.com/ViLab-UCSD/GeoNet" target="_blank"> this link.</a>
    </p>

    <p>
      <b>Metadata Policy:</b> GeoNet also has rich metadata in the form of captions, tags and GPS coordinates for each image available along with the images. Participants may choose to additionally use this metadata for building their algorithms. However, note that <b>no</b> metadata will be provided along with the test set, so the model can only take image inputs at test-time.
    </p>

    <p>
      Registration Link: <a href="https://forms.gle/zSZA1iaPD3mZxjyn7" target="_blank">https://forms.gle/zSZA1iaPD3mZxjyn7.</a>
    </p>

  </div>
</div>


<p><br /></p>
<div class="row" id="details">
  <div class="col-xs-12">
    <h2><u>Challenge Details</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
        <li> <b> Unsupervised Domain Adaptation (UDA) on GeoPlaces: </b> In this challenge, the participants have to devise novel unsupervised domain adaptation methods for improving performance on the task of place recognition across geographies. For the purpose of this challenge, the source domain is fixed as <i>USA</i> and the target as <i>Asia</i> domains. Therefore, the task is to leverage the labeled images from the USA split from the GeoPlaces dataset and improve performance on the Asia images, from which several unlabeled images are also provided. The participants cannot use the labels from the Asia images during training. 
            <ul>
                <li><b>Dataset Download Links: </b> 
                  
                <center>
                <table class="table table-striped" style="width: 50%; margin: auto;">
                <tbody>
                  <tr>
                    <th style="text-align: center;" colspan="3">UDA on GeoPlaces</th>
                  </tr>
                  <tr>
                    <th style="text-align: center;">Images</th>
                    <th style="text-align: center;">Metadata</th>
                    <th style="text-align: center;">Test Data</th>
                  </tr>
                  <tr>
                    <td style="text-align: center;"><a href="https://drive.google.com/file/d/1VeMkGu2kyqRHPSe0Gg7c4qJMdSZUtteS/view?usp=drive_link" style="color: blue;">Link</a></td>
                    <td style="text-align: center;"><a href="https://drive.google.com/file/d/1LQknhiXImFt0L9fyL0lIz-lCx6Wupuxg/view?usp=sharing" style="color: blue;">Link</a></td>
                    <td style="text-align: center;">Unseen</td>
                  </tr>
                  </tbody>
                </table>
              </center>
            </ul>
        </li>
        
        <p><br /></p>            
        <li> <b> Unsupervised Domain Adaptation (UDA) on GeoImNet: </b> In this challenge, the participants have to devise novel unsupervised domain adaptation methods for improving performance on the task of image classification across geographies, where the categories span several objects, places, living organisms etc. For the purpose of this challenge, the source domain is fixed as <i>USA</i> and the target as <i>Asia</i> domains. As with UDA on GeoPlaces, the task is to leverage the labeled images from the USA split from the GeoImnet dataset and improve performance on the Asia images, from which unlabeled images would be provided. The participants cannot use the labels from the Asia images during training. </li>
              <ul>
                  <li><b>Dataset Download Links: </b> 
                    
                  <center>
                  <table class="table table-striped" style="width: 50%; margin: auto;">
                  <tbody>
                    <tr>
                      <th style="text-align: center;" colspan="3">UDA on GeoImNet</th>
                    </tr>
                    <tr>
                      <th style="text-align: center;">Images</th>
                      <th style="text-align: center;">Metadata</th>
                      <th style="text-align: center;">Test Data</th>
                    </tr>
                    <tr>
                      <td style="text-align: center;"><a href="https://drive.google.com/file/d/1XA3g9KuPjKIsVDHahm0T2Wv4_SXL19iV/view?usp=drive_link" style="color: blue;">Link</a></td>
                      <td style="text-align: center;"><a href="https://drive.google.com/file/d/13ny2WvPNlOuUBF-tHQ_YInHqd4e_fBS5/view?usp=sharing" style="color: blue;">Link</a></td>
                      <td style="text-align: center;">Unseen</td>
                    </tr>
                    </tbody>
                  </table>
                </center>
              </ul>
          
              <p><br /></p>    
        <li> <b> Universal Domain Adaptation (UniDA): </b> Universal Domain Adaptation (UniDA) facilitates domain adaptation between source and target domains that have few private classes, in addition to shared classes which are common to both. 
          <!-- Our proposed GeoNet dataset gives us an unique opportunity to design benchmarks for UniDA such that the private categories from the source and the target are a natural reflection of the presence or absence of these categories in the respective geographical domains.  -->
          In this challenge, participants will have to use labeled data from a labeled USA domain as the source, and unlabeled Asia domain as the target, with an additional challenge that the label spaces do not completely overlap during training and testing. 
                <ul>
                    <li><b>Dataset Download Links: </b> 
                      
                    <center>
                    <table class="table table-striped" style="width: 50%; margin: auto;">
                    <tbody>
                      <tr>
                        <th style="text-align: center;" colspan="3">UniDA on GeoUniDA</th>
                      </tr>
                      <tr>
                        <th style="text-align: center;">Images</th>
                        <th style="text-align: center;">Metadata</th>
                        <th style="text-align: center;">Test Data</th>
                      </tr>
                      <tr>
                        <td rowspan="2" style="text-align: center;"><a href="https://drive.google.com/file/d/10hoPOQN7LpLQwlTh7SPk22iGvJbYjnSK/view?usp=drive_link" style="color: blue;">Link</a></td>
                        <td rowspan="2" style="text-align: center;"><a href="https://drive.google.com/file/d/1Zub1SUFijipfIE3USNC6LodaY9F8e-I0/view?usp=sharing" style="color: blue;">Link</a></td>
                        <td style="text-align: center;">Unseen</td>
                      </tr>
                      </tbody>
                    </table>
                  </center>
                </ul>
                <p><br /></p>     
                  
        </li>

            <li>
              <b>Evaluation Metrics:</b> For the UDA challenges on GeoPlaces and GeoImnet, the models will be ranked based on the average Top-3 accuracy on <i> unseen test set </i> from respective tasks. Owing to the label noise and possible equivalent labels in the dataset, we are using Top-3 accuracy and not Top-1 accuracy for this challenge. For the GeoUniDA challenge, following prior work in OpenSet-DA, we use H-acc metric to rank the models for the UniDA task. The H-acc metric is the harmonic mean of the closet-set accuracy (over the seen classes) and the binary open-set accuracy (of rejecting the outlier target test-samples). The H-acc will be evaluated on the unseen test set.
            </li>
            <p><br /></p>
            <center>
              <figure>
                <img src="static/img/site/formula.png" width="200">
            </figure>
            </center>
          <p><br /></p>
            There will be separate leaderboards for all three challenges, and the participants can submit their entries to one or more of them. The test images will be publicly released later in the challenge. 

            <li>
              <b>Challenge Timeline:</b> 
              <table class="table table-striped" style="width: 65%; margin: auto;">
                <tbody>
                  <tr>
                    <td>Challenge Data Released</td>
                    <!-- <td><strike>June 15 2023</strike></td> -->
                    <td> June 15, 2023 </td>
                  </tr>
                  <tr>
                    <td>Challenge Test Set Released</td>
                    <td><s>August 10</s>  August 20, 2023</td>
                  </tr>
                  <tr>
                    <td>Submission Portal Open</td>
                    <td><s>Aug 15</s>  August 20, 2023</td>
                  </tr>
                  <tr>
                    <td>Challenge Submission Deadline</td>
                    <td><s>Aug 25</s>  September 5, 2023</td>
                  </tr>
                  <tr>
                    <td>Workshop Date</td>
                    <!-- <td><strike>June 25 2021 (<span style="background-color:lightcoral;">Day 7 of CVPR 2021</span>)</strike></td> -->
                    <td>Oct 2, 2023 (<span style="background-color:lightcoral;">Day 1 of ICCV 2023</span>)</td>
                  </tr>
                </tbody>
              </table>
            </li>

    </ul>
  </div>
</div>

<p><br /></p>
<div class="row" id="rules">
  <div class="col-xs-12">
    <h2><u>General Guidelines</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">

    <p>
      <b>Registration:</b> 
    </p>
    <ul>
      <li>
        All the teams have to be registered at this link: <a href="https://forms.gle/zSZA1iaPD3mZxjyn7" target="_blank">https://forms.gle/zSZA1iaPD3mZxjyn7.</a> <b>Registering your team is mandatory </b> to participate in the challenge and obtain the test data. 
      </li>
    </ul>

    <p>
      <b>Model Sizes and Data Policy:</b>
    </p>
    <ul>
        <li> 
          Participants can only utilize data from ImageNet-1k for pre-training their models and the provided GeoNet data during training and adaptation. Use of any other kinds of external data, either for pre-training or fine-tuning, is strictly prohibited, which includes the collection or annotation of new data related to the target geographies. If you need any clarifications, please feel free to contact the organizers. 
        </li>

        <li>
          The participants of the challenge can only use the training data from the source domain to train the model. Further, only the unlabeled data from the target <i>train split</i> is to be used during adaptation, and they are not allowed to use any kind of data from the <i>test split</i>. The test split data is provided only to guage/estimate the performance of the trained models.
        </li>

        <li>
          The participants are allowed to use the metadata (except the labels!) provided along with the GeoNet dataset. However, no metadata beyond the raw images will be provided at test-time to evaluate the models, so the methods should be designed so as to take only image inputs during testing. 
        </li>
          
        <li>
          Since this is an unsupervised adaptation challenge, any method that make use of target labels for training their methods will be automatically disqualified from the competition. Only source labels are allowed for use in training, while unlabeled data can be leveraged from target. 
        </li>

        <li>
          To ensure fairness across various resource and compute capabilities, we restrict the models to have a maximum of 300 million parameters in total. This is to help encourage methods that design novel algorithmic solutions as opposed to solely relying on scaling up training with larger models. If ensemble models are used, then the <i>total</i> number of parameters in all of the models should not exceed 300M. 
        </li>
        
        <li>
          If your method uses a pre-trained image generative model as part of the algorithm, then the parameters of the generative model will also be counted towards the 300M limit.
          In case of multi-modal models like CLIP, the parameters from <i>both</i> the vision and language encoders will be counted towards the 300M limit. Note that both the generative model or the CLIP-like models cannot be trained/pre-trained on any additional data apart from ImageNet-1k or provided GeoNet dataset. 
        </li>
    </ul>

    <p>
      <b>Winners and Reproduciblity:</b>
    </p> 
    <ul>

      <li>
        Each team is allowed a maximum of <b>3 submissions</b> to the leaderboard for the duration of the challenge.
      </li>

      <li> The winners will be selected based on the leaderboard performance, with separate winners for the three challenge tracks.   </li>

      <li> The winners of the challenge will be invited to present their winning solution during the workshop. Additionally, they will also be required to submit a brief report summarizing their solution, as well as a working code and trained models that ensure reproduciblity. If the reproduced results do not match those on the leaderboard, the team will be contacted for further clarifications. </li>

    </ul>
  </div>
</div>


<p><br /></p>
<div class="row" id="prizes">
  <div class="col-xs-12">
    <h2><u>Leaderboard and Prizes!</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
        <li>
          <b>Prizes:</b> The total prize money for the challenge would be 9000 USD, split equally between the three challenges. The winner of each challenge will recieve a prize money of 2000 USD, while the runner-up gets 1000 USD. The first and second prize winners from each challenge will be decided based on the leaderboard rankings on the unseen test set. 
        </li>

        <li>
          <b>Leaderboard:</b> Coming soon! In the meanwhile, you can use the test split already provided along with the data to test your models and algorithms.
        </li>
    </ul>
  </div>
</div>

<p><br /></p>
<div class="row" id="contact">
  <div class="col-xs-12">
    <h2><u>Contact</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
        For any questions or clarifications about the challenge, please contact the organizers at <b>geonet *dot* robustness *at* gmail.com</b>. 
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>