
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="iccv, workshop, computer vision, robustness, domain adaptation, transfer learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  <title>GeoNet @ICCV2023</title>
  <meta name="description" content="Robust Computer Vision Across Geographies, ICCV 2023 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Robust Computer Vision Across Geographies"/>
  <meta property="og:url" content="https://geonet-challenge.github.io"/>
  <meta property="og:description" content="Robust Computer Vision Across Geographies, ICCV 2023 Workshop"/>
  <meta property="og:site_name" content="GeoNet Workshop"/>
  <meta property="og:image" content="https://geonet-challenge.github.io/static/img/site/teaser.png"/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Robust Computer Vision Across Geographies Workshop"/>
  <meta name="twitter:image" content="https://geonet-challenge.github.io/static/img/site/teaser.png">
  <meta name="twitter:url" content="https://geonet-challenge.github.io"/>
  <meta name="twitter:description" content="Robust Computer Vision Across Geographies, ICCV 2023 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>1st Workshop on Robust Computer Vision Across Geographies</h1></center>
    <center><h2>ICCV 2023 Workshop</h2></center>
    <!-- <center>June 25, 2021</center> -->
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- <div class="row" id="intro">  
    <div>  
    <img src="static/img/site/teaser.jpg" style="width: 100%; height: auto;"/>
  </div>
</div> -->


<!-- <div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/a.png">
</div>

<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->


<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      This is the first workshop on domain adaptation and robust learning across geographies in computer vision. As part of this workshop, we aim to bring together researchers from across the vision community to foster discussions on the spectrum of challenges posed by geographic bias towards fair and inlcusive computer vision, and the pathway towards building geographically transferable and robust models that counter such biases enabling effective deployment of modern AI technology in low and mid-income societies. 
      
      
      <!-- In recent years, domain adaptation has emerged as an effective technique to alleviate dataset bias during training and improve transferability of vision models to sparsely labeled target domains. However, the geographic origin of data remains a significant source of bias, attributable to several factors of variation between train and test data. Training on geographically biased datasets may cause a model to learn the idiosyncrasies of their geographies, preventing generalization to novel domains with significantly different geographic and demographic composition. Besides robustness, this may have deep impact towards fair and inclusive computer vision. 

      This is the first workshop on natural language and 3D-oriented object understanding of real-world scenes. Our primary goal is to spark research interest in this emerging area, and we set two objectives to achieve this. Our first objective is to bring together researchers interested in natural language and object representations of the physical world. This way, we hope to foster a multidisciplinary and broad discussion on how humans use language to communicate about different aspects of objects present in their surrounding 3D environments. The second objective is to benchmark progress in connecting language to 3D to identify and localize 3D objects with natural language. Tapping on the recently introduced large-scale datasets of <b>ScanRefer</b> and <b>ReferIt3D</b>, we host two benchmark challenges on language-assisted <i>3D localization and identification tasks</i>. The workshop consists of presentations by experts in the field and short talks regarding methods addressing the benchmark challenges designed to highlight the emerging open problems in this area. -->
    </p>
  </div>
</div>

<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Challenges</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      GeoNet robustness challenge details coming soon!
    </p>
    <!-- <ul>      

      <li>
        <strong>3D Object Localization</strong>: to predict a bounding box in a 3D scene corresponding to an object described in natural language
      </li>      
      <li>
        <strong>Fine-grained 3D Object Identification</strong>: to identify a referred object among multiple objects in a 3D scene given natural or spatial-based language
      </li>
    </ul> -->

    <!-- <div class="row" id="tasks">
      <div class="col-md-6">
        <img src="static/img/site/localization.jpg" height="180px"/>
        <p>3D Object Localization</p>
      </div>
      <div class="col-md-6">
        <img src="static/img/site/identification.png" height="180px"/>
        <p>Fine-grained 3D Object Identification</p>
      </div>
    </div> -->

    <!-- <p>
      The challenge leaderboard is online. If you want to join the challenge, see more details here:
    </p>
    <ul>      
      <li>
        <strong><b><a href="http://kaldir.vc.in.tum.de/scanrefer_benchmark/">ScanRefer Challenge</a></b></strong>
      </li>
      <li>
        <strong><b><a href="https://referit3d.github.io/benchmarks.html"> ReferIt3D Challenge</a></b></strong>
      </li>
    </ul> -->
  </div>
</div>

<p><br /></p>
<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <!-- <tr>
          <td>ScanRefer Challenge Submission Deadline</td>
          <td><strike>May 31 2021</strike></td>
        </tr>
        <tr>
          <td>Notification to ScanRefer Challenge Winner</td>
          <td><strike>June 1 2021</td>
        </tr>
        <tr>
          <td>ReferIt3D Challenge Submission Deadline</td>
          <td><strike>June 11 2021</strike></td>
        </tr>
        <tr>
          <td>Notification to ReferIt3D Challenge Winner</td>
          <td><strike>June 12 2021</strike></td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td><strike>June 25 2021 (<span style="background-color:lightcoral;">Day 7 of CVPR 2021</span>)</strike></td>
        </tr> -->
        <tr>
          Coming Soon!
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule (Pacific Time Zone)</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <!-- <tr><td>TBD</td><td>TBD</td></tr> -->
        <!-- <tr>
          <td>Welcome and Introduction</td>
          <td>12:00 - 12:10</td>
        </tr>
        <tr>
          <td><i>From Disembodied to Embodied Grounded Language</i> (<strong>Dhruv Batra</strong>)</td>
          <td>12:10 - 12:40</td>
        </tr>
        <tr>
          <td><i>Generating Animated Videos of Human Activities from Natural Language Descriptions</i> (<strong>Raymond Mooney</strong>)</td>
          <td>12:40 - 13:10</td>
        </tr>
        <tr>
          <td>Winner Talk for ScanRefer</td>
          <td>13:10 - 13:20</td>
        </tr>
        <tr>
          <td><i>Affordances for Action in 3D Spaces</i> (<b>Kristen Grauman</b>)</td>
          <td>13:20 - 13:50</td>
        </tr>
        <tr>
          <td>Break </td>
          <td>13:50 - 14:20</td>
        </tr>
        <tr>
          <td><i>The Semantics and Pragmatics of Reference to 3D Objects</i> (<b>Noah Goodman</b>)</td>
          <td>14:20 - 14:50</td>
        </tr>
        <tr>
          <td>Winner Talk for ReferIt3D</td>
          <td>14:50 - 15:00</td>
        </tr>
        <tr>
          <td><i>Language Grounding Using Neural 3D Scene Representations</i> (<strong>Katerina Fragkiadaki</strong>)</td>
          <td>15:00 - 15:30</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>15:30 - 16:00</td>
        </tr> -->
        Coming Soon!
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.utexas.edu/users/grauman/" target="_blank"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/grauman.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.utexas.edu/users/grauman/">Dr. Kristen Grauman</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin. Her primary research interests are visual recognition and
      visual search and she is well-known for her pioneering works on large-scale image/video retrieval, active learning, active recognition, first-person "egocentric" computer vision, multimodal learning, activity recognition, vision and language, and video summarization. She has also recently led a team of researchers worldwide towards developing a plane-scale egocentric video dataset called Ego4D, which had a significant influence on the landscape of egocentric computer vision.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-2">
    <a href="https://faculty.cc.gatech.edu/~judy/" target="_blank"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/hoffman.jpg"/></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://faculty.cc.gatech.edu/~judy/">Dr. Judy Hoffman</a></b> is an Assistant Professor in the School of Interactive Computing at Georgia Tech, a member of the Machine Learning Center, and a Diversity and Inclusion Fellow. Her research lies at the intersection of computer vision and machine learning with specialization in domain adaptation, transfer learning, adversarial robustness, and algorithmic fairness. She has received numerous awards including NSF CAREER, Google Research Scholar Award (2022), Samsung AI Researcher of the Year Award (2021), NVIDIA female leader in computer vision award (2020), AIMiner top 100 most influential scholars in Machine Learning (2020), MIT EECS Rising Star in 2015, and the NSF Graduate Fellowship. 
      <!-- In addition to her research, she co-founded and continues to advise for Women in Computer Vision, an organization which provides mentorship and travel support for early-career women in the computer vision community.  -->
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="http://www.cs.columbia.edu/~vondrick/" target="_blank"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/vondrick.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="http://www.cs.columbia.edu/~vondrick/"> Dr Carl Vondrick </a></b> is an associate professor of computer science at Columbia University. His research focuses on computer vision and machine learning. His research is supported by the NSF, DARPA, Amazon, and Toyota, and his work has appeared on the national news, such as CNN, NPR, the Associated Press and Stephen Colbert's television show. He received the 2021 NSF CAREER Award, the 2021 Toyota Young Faculty Award, and the 2018 Amazon Research Award. Previously, he was a Research Scientist at Google and he received his PhD from MIT in 2017. 
    </p>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://beerys.github.io/" target="_blank"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/sara.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://beerys.github.io/">Dr. Sara Beery</a></b> is currently a Visiting Researcher at Google working on urban forest monitoring, and will join MIT as an assistant professor in the Faculty of Artificial Intelligence and Decision-Making in EECS in September 2023. Beery received her PhD in computing and mathematical sciences at Caltech in 2022, where she was advised by Pietro Perona. Her research focuses on building computer vision methods that enable global-scale environmental and biodiversity monitoring across data modalities, tackling real-world challenges including strong spatiotemporal correlations, imperfect data quality, fine-grained categories, and long-tailed distributions. She partners with nongovernmental organizations and government agencies to deploy her methods in the wild worldwide and works toward increasing the diversity and accessibility of academic research in artificial intelligence through interdisciplinary capacity building and education.
    </p>
  </div>
</div>
<p><br /></p>

  
</div>


<p><br /></p>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://tarun005.github.io/" target="_blank">
      <img class="people-pic" src="static/img/people/tarun.png" />
    </a>
    <div class="people-name">
      <a href="https://tarun005.github.io/" target="_blank">Tarun Kalluri</a>
      <h6>UC San Diego</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://yuyingyeh.github.io/" target="_blank">
      <img class="people-pic" src="static/img/people/YuYing.png" />
    </a>
    <div class="people-name">
      <a href="https://yuyingyeh.github.io/" target="_blank">Yu-Ying Yeh</a>
      <h6>UC San Diego</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://lear.inrialpes.fr/people/alahari/" target="_blank">
      <img class="people-pic" src="static/img/people/karteek.jpg" />
    </a>
    <div class="people-name">
      <a href="https://lear.inrialpes.fr/people/alahari/" target="_blank">Karteek Alahari</a>
      <h6>Inria Grenoble</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cseweb.ucsd.edu//~mkchandraker/" target="_blank">
      <img class="people-pic" src="static/img/people/manmohan.jpg" />
    </a>
    <div class="people-name">
      <a href="https://cseweb.ucsd.edu//~mkchandraker/" target="_blank">Manmohan Chandraker</a>
      <h6>UC San Diego</h6>
    </div>
  </div>

</div>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <b>geonet *dot* robustness *at* gmail.com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/ECCV2022/">Language for 3D scenes workshop</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>